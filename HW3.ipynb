{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jiangzian96/DS1012/blob/master/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZvRsKxerr_C",
        "colab_type": "text"
      },
      "source": [
        "# Homework 3: Word Embeddings\n",
        "\n",
        "In this homework assignment, we are going to tackle two different exercises involving word embeddings. First, we are going to explore using the semantic orientation method to score words. Second, we will explore the effect of context sizes in building embeddings based on word cooccurrences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRjB4n70e_Nv",
        "colab_type": "text"
      },
      "source": [
        "# Part 0: Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIT93qtzg7rA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install sacremoses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv7TBcl5e_Nf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import sacremoses\n",
        "import tqdm\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0lWzIl3e_N1",
        "colab_type": "text"
      },
      "source": [
        "We're going load a set of 50D word vectors from GloVe. If want to download other versions of GloVe, you can click [here](https://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2INTutRPfCVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://docs.google.com/uc?id=1si-rTb4nALWb-Wah5_tqbs9KXAwE3qhK -O glove.aa\n",
        "!wget https://docs.google.com/uc?id=1_XywxYH56-tHepqxpUfE-Y6xNQSZiu-7 -O glove.ab\n",
        "!wget https://docs.google.com/uc?id=1ISSLqQWL8EO8blT54RKahkDQhtmSp9ae -O glove.ac\n",
        "!wget https://docs.google.com/uc?id=1qHqCKLFO0Zdyhmb_ctd63aaFRLNGiDfB -O glove.ad\n",
        "!cat glove.?? > 'glove.6B.50d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1t4crjzWsOpp",
        "colab_type": "text"
      },
      "source": [
        "We'll load the GloVe embeddings similar to previous labs. `words_to_load` specifies how many word vectors we want to load. The words are saved in frequency order, so specifying 50,000 means that we only want to work with the 50,000 most frequent words from the source corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1MqtauHe_N4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words_to_load = 50000\n",
        "\n",
        "with open('./glove.6B.50d.txt') as f:\n",
        "    loaded_embeddings = np.zeros((words_to_load, 50))\n",
        "    words_to_index = {}\n",
        "    ordered_words = []\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= words_to_load: \n",
        "            break\n",
        "        \n",
        "        s = line.split()\n",
        "        loaded_embeddings[i, :] = np.asarray(s[1:])\n",
        "        words_to_index[s[0]] = i\n",
        "        ordered_words.append(s[0])\n",
        "print(\"GloVe loaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_SUYoKFe_OB",
        "colab_type": "text"
      },
      "source": [
        "Let's look at the embedding for the word \"potato\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_wdFWAXe_ON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loaded_embeddings[words_to_index['potato']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqeYUGvJe_OW",
        "colab_type": "text"
      },
      "source": [
        "# Part 1: The Semantic Orientation Method [40]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTvdJ39Fe_OZ",
        "colab_type": "text"
      },
      "source": [
        "The __semantic orientation__ method of [Turney and Littman 2003](http://doi.acm.org/10.1145/944012.944013) is a method for automatically scoring words along some single semantic dimension like sentiment. It works from a pair of small seed sets of words that represent two opposing points on that dimension.\n",
        "\n",
        "*Some code in this section was adapted from Stanford CS 224U*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EK48UHrke_Oc",
        "colab_type": "text"
      },
      "source": [
        "Here's a sample pair of seed sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtlIac9Ge_Oe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_words = ['good', 'great', 'awesome', 'like', 'love']\n",
        "negative_words = ['bad', 'awful', 'terrible', 'hate', 'dislike']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXaolKIIe_Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SemanticOrientation:\n",
        "    \n",
        "    def __init__(self, seed_pos, seed_neg):\n",
        "        \"\"\"\n",
        "        args: \n",
        "            - seed_pos: list of 'positive' words\n",
        "            - seed_neg: list of 'negative' words\n",
        "        \"\"\"\n",
        "        self.seed_pos = seed_pos\n",
        "        self.seed_neg = seed_neg\n",
        "        seed_pos_indices = [words_to_index[seed] for seed in seed_pos]\n",
        "        seed_neg_indices = [words_to_index[seed] for seed in seed_neg]\n",
        "        \n",
        "        # Get matrices of embeddings for positive and negative words\n",
        "        self.seed_pos_mat = loaded_embeddings[seed_pos_indices]\n",
        "        self.seed_neg_mat = loaded_embeddings[seed_neg_indices]\n",
        "        \n",
        "    def determine_coefficient(self, candidate_words):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            - candidate_words: single word, or list of words\n",
        "        \"\"\"\n",
        "        \n",
        "        # Optionally handle a single word\n",
        "        one_word = isinstance(candidate_words, str)\n",
        "        if one_word:\n",
        "            candidate_words = [candidate_words]\n",
        "\n",
        "        # Retrieve matrix of embeddings for candidate words\n",
        "        candidates_mat = np.array([\n",
        "            loaded_embeddings[words_to_index[candidate_word]]\n",
        "            for candidate_word in candidate_words\n",
        "        ])\n",
        "        # Compute the cosine similarity between the seed word and \n",
        "        #   the candidate word embeddings, and average\n",
        "        pos_sim = cosine_similarity(self.seed_pos_mat, candidates_mat).mean(axis=0)\n",
        "        neg_sim = cosine_similarity(self.seed_neg_mat, candidates_mat).mean(axis=0)\n",
        "        # Compute the difference between the average positive and negative similarities\n",
        "        diff = pos_sim - neg_sim\n",
        "\n",
        "        if one_word:\n",
        "            return diff[0]\n",
        "        else:\n",
        "            return diff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8ho1OPve_Or",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_so = SemanticOrientation(\n",
        "    seed_pos=positive_words,\n",
        "    seed_neg=negative_words,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCokxNHHe_Oy",
        "colab_type": "text"
      },
      "source": [
        "Let's get the coefficients of some words and see if it lines up with our intuition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "MgIfflYpe_O0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_so.determine_coefficient('abhorrent')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnCoEmT4e_O6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_so.determine_coefficient('excellent')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7s5-BXFe_O_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_so.determine_coefficient('amazing')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utlgXcWee_PH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_so.determine_coefficient('okay')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSjJ6ceRe_PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentiment_so.determine_coefficient('atrocious')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm3h-qP29ttp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scatter_and_annotate(ax, x_values, y_values, labels, color):\n",
        "    ax.scatter(x_values, y_values, color=color)\n",
        "    for i, label in enumerate(labels):\n",
        "        ax.text(x_values[i], y_values[i], label, rotation=45)\n",
        "\n",
        "def plot_semantic_orientation(semantic_orientation, words):\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "    ax = fig.gca()\n",
        "    x_values = semantic_orientation.determine_coefficient(semantic_orientation.seed_pos) \n",
        "    x_values = semantic_orientation.determine_coefficient(semantic_orientation.seed_pos) \n",
        "\n",
        "    scatter_and_annotate(\n",
        "        ax=ax,\n",
        "        x_values=semantic_orientation.determine_coefficient(semantic_orientation.seed_pos),\n",
        "        y_values=[0] * len(semantic_orientation.seed_pos),\n",
        "        labels=semantic_orientation.seed_pos,\n",
        "        color=\"blue\",\n",
        "    )\n",
        "    scatter_and_annotate(\n",
        "        ax=ax,\n",
        "        x_values=semantic_orientation.determine_coefficient(semantic_orientation.seed_neg),\n",
        "        y_values=[0] * len(semantic_orientation.seed_neg),\n",
        "        labels=semantic_orientation.seed_neg,\n",
        "        color=\"red\",\n",
        "    )\n",
        "    ax.axhline(0, color=\"gray\")\n",
        "\n",
        "    scatter_and_annotate(\n",
        "        ax=ax,\n",
        "        x_values=semantic_orientation.determine_coefficient(words),\n",
        "        y_values=[-2] * len(words),\n",
        "        labels=words,\n",
        "        color=\"purple\",\n",
        "    )\n",
        "    ax.axhline(-2, color=\"black\")\n",
        "    ax.set_ylim(-2.5, 1)\n",
        "    ax.set_yticks([-2, 0])\n",
        "    ax.set_yticklabels([\"words\", \"seed\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6eXvmqT9GAx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_semantic_orientation(\n",
        "    semantic_orientation=sentiment_so, \n",
        "    words=[\"abhorrent\", \"excellent\", \"amazing\", \"okay\", \"atrocious\"],\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAXq32WIe_PU",
        "colab_type": "text"
      },
      "source": [
        "And sort our vocabulary by its score along the axis. For now, we're only scoring frequent words, since this process can be slow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIXIX_LLApL5",
        "colab_type": "text"
      },
      "source": [
        "### Question 1.1 [10]\n",
        "\n",
        "Using `sentiment_so`, compute the coefficients of all `ordered_words` and show the 10 most positive and 10 most negative words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX3HSrbfe_PV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7x_GhEge_Ph",
        "colab_type": "text"
      },
      "source": [
        "### Question 1.2 [30]\n",
        "\n",
        "1. Create another `SemanticOrientation` object with a different set of contrasting \"positive\" and \"negative\" categories that are unrelated to sentiment (e.g solids vs. liquids, living things vs objects) and corresponding seed words. Have at least 5 words per category.\n",
        "2. Pick another 5 words in either category and show their coefficients. Display them using the plotting code from above.\n",
        "3. Using `ordered_words`, show the 10 most positive and 10 most \"negative\" words.\n",
        "4. Pick 2 words that are unrelated to either category, compute their coefficients, and comment on them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsQhgLeae_Pi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxO7owlte_Py",
        "colab_type": "text"
      },
      "source": [
        "# Part 2: Exploring effect of context size [60 pts]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaVVYaUbe_P0",
        "colab_type": "text"
      },
      "source": [
        "Let's take a step back and assume we are creating word embeddings from scratch.\n",
        "\n",
        "We face many implicit and explicit design decisions in creating distributional word representations. In lecture, we discussed creating vectors using a co-occurence matrix built on neighboring pairs of words. We might suspect, however, that we can get more signal of word similarity by considering larger contexts than pairs of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpwg6x8xs9bP",
        "colab_type": "text"
      },
      "source": [
        "We're going to download two files:\n",
        "\n",
        "1. `sst.txt`, a list of sentences from the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/treebank.html).\n",
        "2. `MTURK-771.csv`, a [word-relatedness dataset](http://www2.mta.ac.il/~gideon/mturk771.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kE54j3Hs98q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://docs.google.com/uc?id=1uDpAm-eoZx-kS7UAPELfH0gGAxekkDVd -O sst.txt\n",
        "!wget http://www2.mta.ac.il/~gideon/datasets/MTURK-771.csv -O MTURK-771.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUXiGZ1Ie_P3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mturk_path = \"./MTURK-771.csv\"\n",
        "sst_path = \"./sst.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqh5wfjHNb91",
        "colab_type": "text"
      },
      "source": [
        "First, we will load the SST data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cM4zMwY9NaPp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_sst(sst_path):\n",
        "    tokenizer = sacremoses.MosesTokenizer()\n",
        "    results = []\n",
        "    for line in tqdm.tqdm_notebook(pd.read_csv(sst_path, sep=\"\\t\")[\"sentence\"].values):\n",
        "        results.append(tokenizer.tokenize(line))\n",
        "    return results\n",
        "data = load_sst(sst_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4ncGmpye_P1",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.1 [45]\n",
        "\n",
        "Implement:\n",
        "* `get_token_frequencies_and_cooccurrences`\n",
        "* `prune_vocabulary`\n",
        "* `build_cooccurrence_mat_from_counts`\n",
        "\n",
        "Each of these will be used in  `build_cooccurrence_matrix`, which generates the co-occurence matrix for a window of arbitrary size and for the vocabulary of `max_vocab_size` most frequent words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGMhTNM5NM5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_cooccurrence_matrix(data, max_vocab_size=20000, context_size=2, verbose=True):\n",
        "    \"\"\" \n",
        "    args:\n",
        "        - data: iterable where each item is a string sentence\n",
        "        - max_vocab_size: maximum vocabulary size\n",
        "        \n",
        "    returns:\n",
        "        - coocur_mat: co-occurrence matrix as a numpy array\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Counting words...\")\n",
        "    start_time = time.time()\n",
        "    tok2freq, cooccur_counts = get_token_frequencies_and_cooccurrences(data, context_size)\n",
        "    if verbose:\n",
        "        print(\"\\tFinished counting %d words in %.5f\" % (len(tok2freq), time.time() - start_time))\n",
        "        print(\"Pruning vocabulary...\")\n",
        "    tok2idx, idx2tok = prune_vocabulary(tok2freq, max_vocab_size)\n",
        "    start_time = time.time()\n",
        "    if verbose:\n",
        "        print(\"\\tFinished pruning vocabulary to %d words in %.5f\" % (len(tok2idx), time.time() - start_time))\n",
        "        print(\"Building co-occurrence matrix...\")\n",
        "    start_time = time.time()\n",
        "    cooccur_mat = build_cooccurrence_mat_from_counts(idx2tok, cooccur_counts)\n",
        "    if verbose:\n",
        "        print(\"\\tFinished building co-occurrence matrix in %.5f\" % (time.time() - start_time))\n",
        "    return cooccur_mat, tok2idx, idx2tok, cooccur_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ai5pnuntGjFz",
        "colab": {}
      },
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def get_token_frequencies_and_cooccurrences(data, context_size):\n",
        "    \"\"\" \n",
        "    Compute token frequencies and cooccurrences from data\n",
        "\n",
        "    args: \n",
        "        - data: a list of list of words (or tokens)\n",
        "        - context_size: the vocabulary size to prune to\n",
        "    \n",
        "    return:\n",
        "        A tuple containing:\n",
        "        - tok2freq: a dictionary mapping from words to count\n",
        "        - cooccur_counts: a dictionary of dictionaries, where\n",
        "\n",
        "              cooccur_counts[word1][word2]\n",
        "\n",
        "          is how often word1 cooccurs with word2 within a given context_size\n",
        "    \"\"\"\n",
        "    tok2freq = defaultdict(int)\n",
        "    cooccur_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for datum in data:\n",
        "        for i, tok in enumerate(datum):\n",
        "            tok2freq[tok] += 1\n",
        "            for k in range(context_size + 1):\n",
        "                # === Exercise Start [1/3] === #\n",
        "                raise NotImplementedError()\n",
        "                # === Exercise End [1/3] === #\n",
        "    return tok2freq, cooccur_counts\n",
        "\n",
        "def prune_vocabulary(tok2freq, max_vocab_size):\n",
        "    \"\"\" \n",
        "    Prune vocab by taking max_vocab_size most frequent words \n",
        "\n",
        "    args: \n",
        "        - tok2freq: a dictionary mapping from words to count\n",
        "        - max_vocab_size: the vocabulary size to prune to\n",
        "    \n",
        "    return:\n",
        "        A tuple containing:\n",
        "        - tok2idx: a dictionary mapping from words to the index in idx2tok\n",
        "        - idx2tok: a list of words (the vocabulary), of length max_vocab_size\n",
        "    \"\"\"\n",
        "    tok_and_freqs = [(k, v) for k, v in tok2freq.items()]\n",
        "    # === Exercise Start [2/3] === #\n",
        "    raise NotImplementedError()\n",
        "    # === Exercise End [2/3] === #\n",
        "    return tok2idx, idx2tok\n",
        "\n",
        "def build_cooccurrence_mat_from_counts(idx2tok, cooccur_counts):\n",
        "    \"\"\" Build a cooccurrence matrix from counts and a vocab\n",
        "    args:\n",
        "        - idx2tok: a list of words (the vocabulary), of length N\n",
        "        - cooccur_counts: a dictionary of dictionaries, where\n",
        "\n",
        "              cooccur_counts[word1][word2]\n",
        "\n",
        "          is how often word1 cooccurs with word2\n",
        "    return:\n",
        "        - mat: an NxN matrix (symmetric) of the occurrence counts of the\n",
        "               words in idx2tok\n",
        "    \"\"\"\n",
        "    vocab_size = len(idx2tok)\n",
        "    mat = np.zeros([vocab_size, vocab_size])\n",
        "    for i in tqdm.tqdm_notebook(range(vocab_size)):\n",
        "        for j in range(i, vocab_size):\n",
        "            # === Exercise Start [3/3] === #\n",
        "            raise NotImplementedError()\n",
        "            # === Exercise End [3/3] === #\n",
        "    return mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfImY8VcNl5z",
        "colab_type": "text"
      },
      "source": [
        "Let's build a cooccurrence matrix with `context_size=2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaIAY8oTEXLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mat, tok2idx, idx2tok, _ = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNe_CAgre_QK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok1 = \"the\"\n",
        "tok2 = \"end\"\n",
        "print(\"'{}' and '{}' co-occur {} times\".format(tok1, tok2, mat[tok2idx[tok1]][tok2idx[tok2]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_iprP4ve_QQ",
        "colab_type": "text"
      },
      "source": [
        "### Question 2.2 [15]\n",
        "\n",
        "We are going to evaluate the effect of varying context size in $\\{1, 2, 3, 4\\}$ (leaving all the other settings the same) on the quality of the learned word embeddings, as measured by performance (Spearman correlation) on the word similarity dataset [MTurk-771](http://www2.mta.ac.il/~gideon/mturk771.html) between human judgments and cosine similarity of the learned word vectors (the coccurrence matrices).\n",
        "\n",
        "In this exercise, you will fill in parts of the code in the `evaluate_word_similarity` function. (Hint: make use of the imports we include at the top of the code block; the solution should be pretty short!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktX17VQ9MjCH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hint: some helpful imports!\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def load_word_similarity_dataset(data_file):\n",
        "    mturk_df = pd.read_csv(mturk_path, names=[\"word1\", \"word2\", \"similarity\"])\n",
        "    return mturk_df\n",
        "\n",
        "\n",
        "def evaluate_word_similarity(mturk_df, mat, tok2idx):\n",
        "    \"\"\" Evaluate the word embeddings by comparing the word similarities \n",
        "        to the word similarity scores from the MTurk-771 data.\n",
        "    Notes:\n",
        "        - In some cases, the words in the MTurk-771 data may not appear in \n",
        "          our vocabulary. In those cases, we will simpy skip over those\n",
        "          MTurk-771 examples.\n",
        "        - Use cosine_similarity to compute word similarities, and use\n",
        "          spearmanr to compute the overall rank correlation.\n",
        "\n",
        "    args:\n",
        "        - mturk_df: DataFrame of MTurk-711 data\n",
        "        - mat: cooccurrence matrix computed from above. These will serve as \n",
        "               our word embeddings.\n",
        "        - idx2tok: a list of words (the vocabulary), of length N\n",
        "\n",
        "    return:\n",
        "        - rho: the rank correlation between the predicted word similarities\n",
        "               and the ground truth word similarities from MTurk-771\n",
        "    \"\"\"\n",
        "    preds = []\n",
        "    trgs = []\n",
        "    n_exs = 0\n",
        "    for row in mturk_df.itertuples():\n",
        "        if row.word1 in tok2idx and row.word2 in tok2idx:\n",
        "            # === Exercise Start [1/2] === #\n",
        "            pred_sim = NotImplemented\n",
        "            # === Exercise End [1/2] === #\n",
        "            preds.append(pred_sim)\n",
        "            trgs.append(row.similarity)\n",
        "            n_exs += 1\n",
        "\n",
        "    # === Exercise Start [2/2] === #\n",
        "    rho = NotImplemented\n",
        "    # === Exercise End [2/2] === #\n",
        "    print(\"Evaluated on %d of %d examples\" % (n_exs, len(mturk_df)))\n",
        "    return rho"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksl3dtOhe_QW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mturk_df = load_word_similarity_dataset(mturk_path)\n",
        "scores = []\n",
        "context_sizes = [1, 2, 3, 4]\n",
        "for context_size in context_sizes:\n",
        "    mat, tok2idx, idx2tok, _ = build_cooccurrence_matrix(data, max_vocab_size=10000, context_size=context_size)\n",
        "    score = evaluate_word_similarity(mturk_df, mat, tok2idx)\n",
        "    scores.append(score)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHOxuRuYBioi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "score = evaluate_word_similarity(mturk_df, mat, tok2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7ZMgciye_Qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(context_sizes, scores, marker='o')\n",
        "plt.xlabel(\"Context size\")\n",
        "plt.ylabel(\"Word similarity correlation\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvvTXLbne_Qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}